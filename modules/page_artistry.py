# modules/page_artistry.py (æœ€ç»ˆäº¤äº’ä¸èµ„æºä¿®å¤ç‰ˆ)

import streamlit as st
import pandas as pd
import json
import os  # å¯¼å…¥osæ¨¡å—ä»¥æ£€æŸ¥æ–‡ä»¶è·¯å¾„
from .visualizations import generate_wordcloud_text, create_and_show_wordcloud, get_proxied_image_url


# --- è¯¦æƒ…é¡µæ¸²æŸ“å‡½æ•° ---
# modules/page_artistry.py

# ... (imports ä¿æŒä¸å˜) ...

# --- ã€æœ€ç»ˆä¿®å¤ä¸åŠŸèƒ½è¡¥å…¨ç‰ˆã€‘è¯¦æƒ…é¡µæ¸²æŸ“å‡½æ•° ---
def render_single_movie_details(df_top250):
    """
    æ¸²æŸ“å•ç‰‡æ·±åº¦è§£è¯»æ¨¡å—ã€‚
    æ­¤ç‰ˆæœ¬æ¢å¤äº†â€œå½±è¿·å¿ƒå£°â€å’Œâ€œè‰ºæœ¯å…³è”æ¨èâ€æ¨¡å—ã€‚
    """
    # 1. ä»session_stateè·å–ID
    douban_id = st.session_state.get('selected_movie_douban_id')
    if not douban_id:
        return

    # 2. è¿”å›æŒ‰é’®é€»è¾‘
    if st.button("â¬…ï¸ è¿”å›æµ·æŠ¥å¢™"):
        st.session_state.selected_movie_douban_id = None
        st.rerun()

    # 3. å®‰å…¨åœ°è·å–ç”µå½±è¡Œ
    movie_series = df_top250[df_top250['douban_id'] == douban_id]
    if movie_series.empty:
        st.error(f"é”™è¯¯ï¼šåœ¨æ•°æ®ä¸­æ‰¾ä¸åˆ° douban_id ä¸º {douban_id} çš„ç”µå½±ã€‚")
        return
    movie = movie_series.iloc[0]

    # --- 4. è¯¦æƒ…é¡µUIæ¸²æŸ“ (ä¸ŠåŠéƒ¨åˆ†) ---
    st.title(f"{movie.get('name_cn', 'æœªçŸ¥ç”µå½±')} ({movie.get('name_en', '')})")

    col1, col2 = st.columns([1, 2])
    with col1:
        st.image(get_proxied_image_url(movie.get('cover_url')), use_container_width=True)

    with col2:
        c1, c2 = st.columns(2)
        c1.metric("è±†ç“£è¯„åˆ†", f"{movie.get('score', 0.0):.1f} â­")
        c2.metric("è¯„ä»·äººæ•°", f"{int(movie.get('comment_count', 0)):,}")

        st.markdown(f"**å¯¼æ¼”**: {', '.join(movie.get('directors_list', [])) or 'æš‚æ— æ•°æ®'}")
        st.markdown(f"**ç±»å‹**: {', '.join(movie.get('genres_list', [])) or 'æœªçŸ¥'}")
        st.markdown(
            f"**ä¸Šæ˜ å¹´ä»½**: {int(movie.get('year')) if pd.notna(movie.get('year')) and movie.get('year') > 0 else 'æœªçŸ¥'}")

        if pd.notna(movie.get('quote')):
            st.markdown(f"> {movie.get('quote')}")

        with st.expander("æŸ¥çœ‹å‰§æƒ…ç®€ä»‹"):
            st.write(movie.get('synopsis', 'æš‚æ— ç®€ä»‹ã€‚'))

    st.markdown("---")  # æ·»åŠ åˆ†å‰²çº¿

    # --- 5. ã€æ¢å¤ã€‘å½±è¿·å¿ƒå£° (çƒ­é—¨è¯„è®º) æ¨¡å— ---
    st.subheader("ğŸ’¬ å½±è¿·å¿ƒå£°")
    # æ­¤å¤„å‡è®¾ hot_comments_json åˆ—åœ¨ data_loader.py ä¸­å·²è¢«æ­£ç¡®åŠ è½½
    try:
        # hot_comments_json æ˜¯ä¸€ä¸ªJSONå­—ç¬¦ä¸²ï¼Œéœ€è¦ç”¨json.loadsè§£æ
        comments_str = movie.get('hot_comments_json')
        if pd.notna(comments_str):
            comments_list = json.loads(comments_str)
            if comments_list:
                for comment in comments_list[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
                    st.info(f"**{comment.get('author', 'è±†å‹')}**: {comment.get('comment', '')}")
            else:
                st.info("æš‚æ— çƒ­é—¨è¯„è®ºã€‚")
        else:
            st.info("æš‚æ— çƒ­é—¨è¯„è®ºã€‚")
    except Exception as e:
        st.warning(f"çƒ­é—¨è¯„è®ºè§£æå¤±è´¥ï¼Œå¯èƒ½æ ¼å¼æœ‰è¯¯ã€‚é”™è¯¯: {e}")
        st.info("æš‚æ— çƒ­é—¨è¯„è®ºã€‚")

    st.markdown("---")  # æ·»åŠ åˆ†å‰²çº¿

    # --- 6. ã€æ¢å¤ã€‘è‰ºæœ¯å…³è”æ¨è æ¨¡å— ---
    st.subheader("ğŸ§­ æ¢ç´¢æ›´å¤š")

    directors = movie.get('directors_list', [])
    if directors:
        related_movies = df_top250[
            (df_top250['directors_list'].apply(lambda x: any(d in x for d in directors))) &
            (df_top250['douban_id'] != douban_id)
            ]
        if not related_movies.empty:
            st.markdown("**å¸ˆå‡ºåŒé—¨:**")
            # ä½¿ç”¨æ›´å°çš„åˆ—æ•°ä»¥é¿å…å›¾ç‰‡è¿‡å°
            cols = st.columns(min(5, len(related_movies)))
            for i, (_, rel_movie) in enumerate(related_movies.head(5).iterrows()):
                with cols[i]:
                    st.image(get_proxied_image_url(rel_movie['cover_url']), caption=rel_movie['name_cn'])
        else:
            st.info("åœ¨æœ¬åº“ä¸­æœªæ‰¾åˆ°è¯¥å¯¼æ¼”çš„å…¶ä»–ä½œå“ã€‚")
    else:
        st.info("ç”±äºç¼ºå°‘å¯¼æ¼”æ•°æ®ï¼Œæ— æ³•è¿›è¡Œâ€œå¸ˆå‡ºåŒé—¨â€æ¨èã€‚")


# ... render_page_artistry å‡½æ•°ä¿æŒä¸å˜ ...

# --- ä¸»é¡µé¢æ¸²æŸ“å‡½æ•° ---
def render_page_artistry(df_top250):
    if st.session_state.get('selected_movie_douban_id') is not None:
        render_single_movie_details(df_top250);
        return

    st.title("ğŸ›ï¸ è‰ºæœ¯æ®¿å ‚")
    st.caption("è±†ç“£ç”µå½±Top250å…¨æ™¯ç”»å·")
    st.markdown("---")

    st.header("æ®¿å ‚ä¸»é¢˜ï¼šå‰§æƒ…åŸºå› å›¾è°±")
    col1, col_center, col3 = st.columns([0.1, 3, 0.1])
    with col_center:
        with st.spinner("æ­£åœ¨ç»˜åˆ¶å‰§æƒ…åŸºå› å›¾è°±..."):
            processed_text = generate_wordcloud_text(df_top250['synopsis'], "assets/stopwords.txt")

            # --- ã€æœ€ç»ˆè°ƒç”¨æ–¹å¼ã€‘åªä¼ é€’ä¸¤ä¸ªå‚æ•° ---
            create_and_show_wordcloud(
                processed_text,
                colormap='cividis'  # ä½ å¯ä»¥åœ¨è¿™é‡Œåˆ‡æ¢è‰²ç›˜ï¼Œå¦‚ 'gist_heat'
            )

    st.markdown("---")
    st.header("ç»å…¸æµ·æŠ¥å¢™")
    sorted_df = df_top250.sort_values('ranking')
    movies_per_row = 6

    for i in range(0, len(sorted_df), movies_per_row):
        cols = st.columns(movies_per_row)
        for j, col in enumerate(cols):
            if i + j < len(sorted_df):
                movie = sorted_df.iloc[i + j]
                with col:
                    st.image(get_proxied_image_url(movie.get('cover_url')), use_container_width=True)

                    # ä½¿ç”¨ douban_id ç”Ÿæˆå”¯ä¸€çš„key
                    if st.button(f"#{int(movie.get('ranking', 0))} {movie.get('name_cn', '')}",
                                 key=f"btn_movie_{movie['douban_id']}",
                                 use_container_width=True):
                        # ç‚¹å‡»æŒ‰é’®æ—¶ï¼Œè®¾ç½®session_stateå¹¶è§¦å‘é‡ç»˜
                        st.session_state['selected_movie_douban_id'] = movie['douban_id']
                        st.rerun()